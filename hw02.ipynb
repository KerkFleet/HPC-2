{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# EECS 469/569: Homework 2\n",
    "## OpenMP\n",
    "## Single Node Performance of Roaring Thunder\n",
    "### Due: Sunday, Oct. 17 *before* midnight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "This is a partner assignment (you should work together on all sections); if there are an odd number of students there can be exactly one group of three. A ***formal*** report is **not** required for this homework. The results should be included where noted in this Jupyter notebook by the keyword **DELIVERABLE**. All supporting files required to run the Jupyter notebook must be included in your submission. All figures should be clear, with axes labeled, including a legend, and a caption. All figures should have a short description and referenced in text in the context of the assignment. This assignment will be graded tougher than HW01, make sure you answer **WHY** things happened, not just **WHAT**. \n",
    "\n",
    "Copy this Jupyter notebook (rename it to hw02_yourlastnames) to its own folder in your user space on the Roaring Thunder cluster and only include files relevant to this assignment in the folder. There is information in Lecture Slides 6 and 7 on how to run the Jupyter notebook on the cluster. \n",
    "\n",
    "**(-5 points if not on time) DELIVERABLE:** Email Dr. Hansen with your partner for the assignment before midnight on Monday, Oct. 11 (CC your partner, only send one email per group).  DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "1. familiarize the student with OpenMP (Open Multiprocessing);\n",
    "2. analyze overhead required for OpenMP; and\n",
    "3. apply knowledge of OpenMP to speedup the linear algebra code from Homework 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **(5 points) <span style=\"color:red;\">FINAL DELIVERABLE:**</span> ***After*** you have completed the entire assignment, write a few paragraphs on your main takeaways from the assignment. **Clearly state** how the work was split up between you and your partner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. (Optional) Introduction to OpenMP\n",
    "\n",
    "**This section is optional,** there are no deliverables or points associated with this section. If you are having issues with OpenMP, this may be of use to you to get a refresher. \n",
    "\n",
    "1. Go to [HPC Training Moodle](https://www.hpc-training.org/xsede/moodle/), create an account, and take the Introduction to OpenMP course. There are many other intro courses that may assist you as we continue with the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. OpenMP Overhead\n",
    "\n",
    "The process of forking/joining threads is not free! There is an overhead (in terms of time/CPU cycles) involved in each OpenMP directive and clause. This section will explore the overhead of these different OpenMP constructs using the [Edinburgh Parallel Computing Centre (EPCC) OpenMP MicroBenchmark Suite](https://www.epcc.ed.ac.uk/research/computing/performance-characterisation-and-benchmarking).\n",
    "\n",
    "1. Read the paper \"Measuring Synchronisation and Scheduling Overheads in OpenMP\" on D2L. \n",
    "2. Make sure that you have copied the `syncbench` folder. \n",
    "3. Compile and run the `syncbench` microbenchmark using the provided makefile and SLURM file (you may need to do this in a separate command line window, from the syncbench folder). \n",
    "```bash\n",
    "make syncbench\n",
    "sbatch hw2_sync.slurm\n",
    "```\n",
    "4. Once the batch file has completed, read through the output of the benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points) <span style=\"color:red;\">DELIVERABLE:**</span> Write a one paragraph summary of the paper. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(15 points) <span style=\"color:red\">DELIVERABLE:**</span> Create a table with the average overhead time, and discuss the overhead of the following OpenMP directives: parallel, for, parallel for, barrier, critical, atomic. \n",
    "\n",
    "|Directive   |Overhead Time Ms|\n",
    "|:-----------|:---------------|\n",
    "|Parallel    | 12.788068|\n",
    "|For         | 6.595064 |\n",
    "|Parallel For| 12.128117|\n",
    "|Barrier     | 6.681943 |\n",
    "|Critical    | 0.551266 |\n",
    "|Atomic      | 0.098452 | \n",
    "\n",
    "The sections with parallel took the longest overhead time beacause the threads have to be spawned at that point. With critcal and atomic the parallel section is already defined and there is very little overhead required. while the barrier is included in a parallel section each thread has to execute the statement. The for has a smaller overhead than the parallel for because similar to the critcal and atomic the threads ar already spwaned. The for does have overhead becasue it has to divide and assign the work among the threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. OpenMP Linear Algebra\n",
    "\n",
    "## ⚠️⚠️ **WARNING:** All deliverables must be executed on a node obtained through SLURM for this section ⚠️⚠️\n",
    "\n",
    "In this section, we will begin to explore the speedup gained through OpenMP parallelism using your linear algebra code from Homework 1. Start by making copies of the working versions of your ***WORKING*** Homework 1 code (if you had errors, you need to fix it):\n",
    "* Matrix-Matrix Multiply Transpose: mat_mat_omp.c\n",
    "* Matrix-Vector Product: mat_vec_omp.c\n",
    "* Dot Product: dot_omp.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 OpenMP Matrix-Matrix Product\n",
    "1. Add an `omp parallel for` section to your outermost matrix multiplication loop. \n",
    "    * Recall that only the outermost loop variable is made private\n",
    "    * Ensure thread safety of your result matrix $\\mathbf{C}$\n",
    "    * While testing, you may use the head node of the cluster as long as you use a small matrix size $N$ and number of threads $T$\n",
    "    * To ensure your code is correct, compare the output to a known $i,j$ from Homework 1\n",
    "2. Using a dedicated SLURM node (`--nodes=1`, `--ntasks-per-node=40`), run and time the matrix-matrix product using a matrix size of $N=4096$ and different numbers of threads $T = {2,4,8,16,32,40}$. \n",
    "    * HINT: you may want to write a Bash for-loop in your SLURM file that sets the number of threads and then runs the program. \n",
    "    * HINT: you may want to add an additional column in the output .csv file to specify the number of threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_NUM_THREADS=10\n",
      "10\r\n"
     ]
    }
   ],
   "source": [
    "%env OMP_NUM_THREADS=10\n",
    "!echo $OMP_NUM_THREADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed a 1024 x 1024 matrix multiply transpose in 4.338874 seconds\r\n",
      "Number of floating point operations = 2 * 1024^3 = 2147483648\r\n",
      "Flops = 4.949404e+08\r\n",
      "Element from Matrix C[4][5]: 74924.664062\r\n"
     ]
    }
   ],
   "source": [
    "!gcc matrix_multiply_transpose_serial.c -o matmult_s -lm -fopenmp\n",
    "!./matmult_s 1024 non-parallel.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed a 1024 x 1024 matrix multiply transpose in 0.603840 seconds\r\n",
      "Number of floating point operations = 2 * 1024^3 = 2147483648\r\n",
      "Flops = 3.556381e+09\r\n",
      "Element from Matrix C[4][5]: 74924.664062\r\n"
     ]
    }
   ],
   "source": [
    "!gcc matrix_multiply_transpose_parallel.c -o matmult_p -lm -fopenmp\n",
    "!./matmult_p 1024 parallel.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">**(30 points) DELIVERABLE:**</span> Link to your final '.c' code here: []()\n",
    "\n",
    "Create three figures that have $T$ on the x-axis, and on the y-axis:\n",
    "* parallel speedup (Use your $T=1$ time from Homework 1 from the serial version of the transpose code. If you had an error in your transpose, you should correct it and get a new time.)\n",
    "* floating point operations per second (FLOPs)\n",
    "* execution time\n",
    "\n",
    "***USE AN APPROPRIATE SI PREFIX FOR YOUR Y-AXES!*** Discuss in one paragraph per figure the impact of OpenMP and the number of threads on algorithm performance. ***WHY*** do you think you are seeing the results you are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Optimized Matrix-Matrix Product\n",
    "\n",
    "Using any OpenMP or other methods, for $T=40$ and $N=4096$, obtain the fastest possible time for your matrix-matrix product (while still getting the correct solution). \n",
    "\n",
    "**(5 points, based on final speed) <span style=\"color:red\">DELIVERABLE:**</span> Describe how you obtained your fastest result. Link to your final '.c' code here: []()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed a 4096 x 4096 matrix multiply transpose in 8.791791 seconds\r\n",
      "Number of floating point operations = 2 * 4096^3 = 137438953472\r\n",
      "Flops = 1.563265e+10\r\n",
      "Element from Matrix C[4][5]: 258400.421875\r\n"
     ]
    }
   ],
   "source": [
    "!gcc matrix_multiply_transpose_speed.c -o matmult_f -O2 -lm -fopenmp\n",
    "!./matmult_f 4096 parallel.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Comparison of Different Linear Algebra Algorithms\n",
    "\n",
    "In the prior section we explored matrix-matrix multiply, which is an $N^3$ algorithm on $N^2$ data. In this section, you will explore the scalability of the dot product ($N$ operations on $N$ data) and the matrix-vector product ($N^2$ operations on $N^2$ data). \n",
    "\n",
    "1. Speedup your matrix-vector product and dot product using OpenMP. Ensure you get the same answer as the serial version. \n",
    "2. Using a dedicated SLURM node (`--nodes=1`, `--ntasks-per-node=40`), design an experiment to evaluate the scalability of the two algorithms with respect to number of threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed a 4096 x 4096 matrix vector multiply in 0.045802 seconds\r\n",
      "Number of floating point operations = 2 * 4096^3 = 137438953472\r\n",
      "Flops = 3.000713e+12\r\n",
      "value 4 from Matc: 279472.218750"
     ]
    }
   ],
   "source": [
    "!gcc ./matrix_vector_multiply.c -o matvec -fopenmp -lm\n",
    "!./matvec 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed a 4096 x 4096 matrix vector multiply in 0.003569 seconds\r\n",
      "Number of floating point operations = 2 * 4096^3 = 137438953472\r\n",
      "Flops = 3.851115e+13\r\n",
      "value 4 from Matc: 279472.218750"
     ]
    }
   ],
   "source": [
    "!gcc ./matrix_vector_multiply_speed.c -o matvec_s -fopenmp -lm -O3\n",
    "!./matvec_s 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(20 points) <span style=\"color:red\">DELIVERABLE:** </span>Design an experiment to evaluate the scalability of the two algorithms with respect to number of threads. Compare the two algorithms with the matrix-matrix product. Include any discussion and figures required to support your conclusions. Include a link to your commented source code here: []()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Other OpenMP Stuff\n",
    "\n",
    "## 4.1 More OpenMP\n",
    "\n",
    "Read the latest [OpenMP 5.0 \"cheat sheet\"](https://www.openmp.org/wp-content/uploads/OpenMPRef-5.0-111802-web.pdf). Find a directive or clause that we did not use in class or this assignment, research what it does, and implement a small test case to show its operation. **Do not just copy an example from online.** \n",
    "\n",
    "**(10 points) <span style=\"color:red\">DELIVERABLE</span>:** Link your '.c' code here: []()\n",
    "1. Write a paragraph on the new OpenMP directive/clause that you used, what it does, and how you used it in your code.\n",
    "2. Discuss with another group that chose a different OpenMP directive/clause than you. Write a few sentences about it and which group (by name of team members). \n",
    "\n",
    "## 4.2 Solve a Cool Problem with OpenMP\n",
    "\n",
    "Solve any problem that interests you using OpenMP. \n",
    "\n",
    "**(10 points) <span style=\"color:red\">DELIVERABLE:**</span> Describe what problem you chose and how you accelerated it using OpenMP. Discuss your problem with a different group than **Deliverable 4.1** and describe their problem in a few sentences, and which group (by name of team members). Link your '.c' code here: []()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBMISSION INSTRUCTIONS\n",
    "\n",
    "Zip your .ipynb, all source files (you do not need to submit syncbench), and any necessary data files as 'HW02_XX.zip' (or .7z), where 'XX' is your HW02 group number. Email the .zip file to Dr. Hansen, one per group, and CC your partner. You need to ensure that all figures properly show up in your .ipynb that you email to Dr. Hansen. This needs to be in Dr. Hansen's inbox before midnight on Sunday, Oct. 17. \n",
    "\n",
    "Submit a .pdf (e.g., print via Chrome as .pdf) to the D2L dropbox before the due date as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
